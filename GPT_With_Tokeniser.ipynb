{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -O input.txt https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
        "\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlr2TWxenx2l",
        "outputId": "1ea54328-e94a-4873-bb67-d2d753ef004d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-29 19:30:57--  https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4573338 (4.4M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   4.36M  17.8MB/s    in 0.2s    \n",
            "\n",
            "2025-03-29 19:30:57 (17.8 MB/s) - ‘input.txt’ saved [4573338/4573338]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))\n",
        "\n",
        "print(text[1000:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QVAO98iLkox",
        "outputId": "4ad79db1-7037-46e0-9ee3-6d2a394e2cb8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  4573338\n",
            "Second Citizen:\n",
            "Would you proceed especially against Caius Marcius?\n",
            "\n",
            "All:\n",
            "Against him first: he's a very dog to the commonalty.\n",
            "\n",
            "Second Citizen:\n",
            "Consider you what services he has done for his country?\n",
            "\n",
            "First Citizen:\n",
            "Very well; and could be content to give him good\n",
            "report fort, but that he pays himself with being proud.\n",
            "\n",
            "Second Citizen:\n",
            "Nay, but speak not maliciously.\n",
            "\n",
            "First Citizen:\n",
            "I say unto you, what he hath done famously, he did\n",
            "it to that end: though soft-conscienced men can be\n",
            "content to say it was for his country he did it to\n",
            "please his mother and to be partly proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "Second Citizen:\n",
            "What he cannot help in his nature, you account a\n",
            "vice in him. You must in no way say he is covetous.\n",
            "\n",
            "First Citizen:\n",
            "If I must not, I need not be barren of accusations;\n",
            "he hath faults, with surplus, to tire in repetition.\n",
            "What shouts are these? The other side o' the city\n",
            "is risen: why stay we prating here? to the Capitol!\n",
            "\n",
            "All:\n",
            "Come, come.\n",
            "\n",
            "First C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "import math"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCXlzjenJzIH",
        "outputId": "8de88791-0c36-4813-ef12-55de2d707237"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f\"Loaded {len(tokens)} tokens\")\n",
        "        print(f\"1 epoch = {len(tokens) // (B * T)} batches\")\n",
        "\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buffer = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
        "        x = (buffer[:-1]).view(B, T) # inputs\n",
        "        y = (buffer[1:]).view(B, T) # targets\n",
        "\n",
        "        self.current_position += B * T\n",
        "\n",
        "        if self.current_position + B * T > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "8MRD_DM17sNN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model hyperparameters\n",
        "n_embed = 768          # Size of embeddings\n",
        "n_head = 12            # Number of attention heads\n",
        "n_layer = 12           # Number of transformer layers\n",
        "batch_size = 4         # Batch size\n",
        "block_size = 1024      # Context length\n",
        "max_iters = 1005       # Training iterations\n",
        "eval_interval = 500    # Evaluation interval\n",
        "learning_rate = 5e-4   # Learning rate\n",
        "eval_iters = 200       # Evaluation steps for validation\n",
        "dropout = 0.1          # Dropout probability\n",
        "\n",
        "vocab_size = 50304     # Vocabulary size :: Originally = 50257, update to nice number 50304 -> divisible by 128\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "\n",
        "print(f\"Using device {device}\")\n"
      ],
      "metadata": {
        "id": "z8NEKZqAXEo8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9743d0b4-7ff4-489b-a1c2-cef52e905585"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CasualSelfAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        assert n_embed % n_head == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.n_embed = n_embed\n",
        "\n",
        "        self.c_attn = nn.Linear(n_embed, 3 * n_embed) # for all 3 (key, query, value)\n",
        "        self.c_proj = nn.Linear(n_embed, n_embed)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embed, dim = 2) # 3 -> (B, T, n_embed)\n",
        "\n",
        "        head_size = C // self.n_head\n",
        "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "\n",
        "        # Flash Attention\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Reshape back to (B, T, n_embed)\n",
        "        y = self.c_proj(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(n_embed, 4 * n_embed)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * n_embed, n_embed)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        # self.attention = MultiHeadAttention(n_head, head_size) # 4 heads of 8-dimensional self-attention\n",
        "        # self.fforward = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.attention = CasualSelfAttention()\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "        self.mlp = MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.ln1(x))\n",
        "        # x = x + self.fforward(self.ln2(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        # self.blocks = nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.language_model_head = nn.Linear(n_embed, vocab_size, bias = False)\n",
        "\n",
        "        self.language_model_head.weight = self.token_embedding_table.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= block_size, f\"Sequence length {T} exceeds block size {block_size}\"\n",
        "\n",
        "        token_embeddings = self.token_embedding_table(idx) #(Batch, Time, Channel)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(0, T, dtype = torch.long, device = idx.device)) #(Time, Channel)\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.language_model_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx -> (B, T)\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:] # (B, T)\n",
        "                logits, loss = self(idx_cond) # (B, T, C)\n",
        "                logits = logits[:, -1, :] # last time step only | becomes (B, C)\n",
        "                prob = F.softmax(logits, dim = -1)\n",
        "                idx_next = torch.multinomial(prob, num_samples = 1) # predicted | (B, 1)\n",
        "                idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
        "        return idx\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        if master_process:\n",
        "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "\n",
        "        if master_process:\n",
        "            print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device = device)\n",
        "# model = torch.compile(model) # Works with A100, not much help for T4\n",
        "\n",
        "# logits, loss = model(x_batch, y_batch)\n",
        "\n",
        "# print(logits.shape)\n",
        "# print(loss)\n",
        "\n",
        "# idx = torch.zeros((1, 1), dtype = torch.long).to(device)\n",
        "# print(decode(model.generate(idx, max_new_tokens = 100)[0].tolist()))"
      ],
      "metadata": {
        "id": "RrahGtfZeDzc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2vazQHn8Wa8",
        "outputId": "6af1adc0-a939-4418-91fd-27aa086f7e42"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 124475904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "num_params = count_parameters(model)\n",
        "print(f\"The model has {num_params} trainable parameters.\")\n",
        "\n",
        "def count_parameters_by_layer(model):\n",
        "    total_params = 0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            num_params = param.numel()\n",
        "            print(f\"Layer: {name}, Parameters: {num_params}\")\n",
        "            total_params += num_params\n",
        "    print(f\"Total Trainable Parameters: {total_params}\")\n",
        "\n",
        "count_parameters_by_layer(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zp183kUO9GG5",
        "outputId": "f6ad68b1-293b-4551-ce27-0d22dfeae4c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 124439808 trainable parameters.\n",
            "Layer: token_embedding_table.weight, Parameters: 38597376\n",
            "Layer: position_embedding_table.weight, Parameters: 786432\n",
            "Layer: blocks.0.ln1.weight, Parameters: 768\n",
            "Layer: blocks.0.ln1.bias, Parameters: 768\n",
            "Layer: blocks.0.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.0.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.0.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.0.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.0.ln2.weight, Parameters: 768\n",
            "Layer: blocks.0.ln2.bias, Parameters: 768\n",
            "Layer: blocks.0.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.0.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.0.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.0.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.1.ln1.weight, Parameters: 768\n",
            "Layer: blocks.1.ln1.bias, Parameters: 768\n",
            "Layer: blocks.1.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.1.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.1.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.1.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.1.ln2.weight, Parameters: 768\n",
            "Layer: blocks.1.ln2.bias, Parameters: 768\n",
            "Layer: blocks.1.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.1.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.1.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.1.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.2.ln1.weight, Parameters: 768\n",
            "Layer: blocks.2.ln1.bias, Parameters: 768\n",
            "Layer: blocks.2.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.2.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.2.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.2.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.2.ln2.weight, Parameters: 768\n",
            "Layer: blocks.2.ln2.bias, Parameters: 768\n",
            "Layer: blocks.2.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.2.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.2.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.2.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.3.ln1.weight, Parameters: 768\n",
            "Layer: blocks.3.ln1.bias, Parameters: 768\n",
            "Layer: blocks.3.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.3.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.3.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.3.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.3.ln2.weight, Parameters: 768\n",
            "Layer: blocks.3.ln2.bias, Parameters: 768\n",
            "Layer: blocks.3.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.3.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.3.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.3.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.4.ln1.weight, Parameters: 768\n",
            "Layer: blocks.4.ln1.bias, Parameters: 768\n",
            "Layer: blocks.4.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.4.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.4.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.4.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.4.ln2.weight, Parameters: 768\n",
            "Layer: blocks.4.ln2.bias, Parameters: 768\n",
            "Layer: blocks.4.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.4.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.4.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.4.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.5.ln1.weight, Parameters: 768\n",
            "Layer: blocks.5.ln1.bias, Parameters: 768\n",
            "Layer: blocks.5.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.5.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.5.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.5.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.5.ln2.weight, Parameters: 768\n",
            "Layer: blocks.5.ln2.bias, Parameters: 768\n",
            "Layer: blocks.5.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.5.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.5.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.5.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.6.ln1.weight, Parameters: 768\n",
            "Layer: blocks.6.ln1.bias, Parameters: 768\n",
            "Layer: blocks.6.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.6.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.6.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.6.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.6.ln2.weight, Parameters: 768\n",
            "Layer: blocks.6.ln2.bias, Parameters: 768\n",
            "Layer: blocks.6.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.6.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.6.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.6.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.7.ln1.weight, Parameters: 768\n",
            "Layer: blocks.7.ln1.bias, Parameters: 768\n",
            "Layer: blocks.7.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.7.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.7.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.7.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.7.ln2.weight, Parameters: 768\n",
            "Layer: blocks.7.ln2.bias, Parameters: 768\n",
            "Layer: blocks.7.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.7.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.7.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.7.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.8.ln1.weight, Parameters: 768\n",
            "Layer: blocks.8.ln1.bias, Parameters: 768\n",
            "Layer: blocks.8.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.8.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.8.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.8.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.8.ln2.weight, Parameters: 768\n",
            "Layer: blocks.8.ln2.bias, Parameters: 768\n",
            "Layer: blocks.8.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.8.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.8.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.8.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.9.ln1.weight, Parameters: 768\n",
            "Layer: blocks.9.ln1.bias, Parameters: 768\n",
            "Layer: blocks.9.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.9.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.9.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.9.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.9.ln2.weight, Parameters: 768\n",
            "Layer: blocks.9.ln2.bias, Parameters: 768\n",
            "Layer: blocks.9.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.9.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.9.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.9.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.10.ln1.weight, Parameters: 768\n",
            "Layer: blocks.10.ln1.bias, Parameters: 768\n",
            "Layer: blocks.10.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.10.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.10.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.10.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.10.ln2.weight, Parameters: 768\n",
            "Layer: blocks.10.ln2.bias, Parameters: 768\n",
            "Layer: blocks.10.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.10.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.10.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.10.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.11.ln1.weight, Parameters: 768\n",
            "Layer: blocks.11.ln1.bias, Parameters: 768\n",
            "Layer: blocks.11.attention.c_attn.weight, Parameters: 1769472\n",
            "Layer: blocks.11.attention.c_attn.bias, Parameters: 2304\n",
            "Layer: blocks.11.attention.c_proj.weight, Parameters: 589824\n",
            "Layer: blocks.11.attention.c_proj.bias, Parameters: 768\n",
            "Layer: blocks.11.ln2.weight, Parameters: 768\n",
            "Layer: blocks.11.ln2.bias, Parameters: 768\n",
            "Layer: blocks.11.mlp.c_fc.weight, Parameters: 2359296\n",
            "Layer: blocks.11.mlp.c_fc.bias, Parameters: 3072\n",
            "Layer: blocks.11.mlp.c_proj.weight, Parameters: 2359296\n",
            "Layer: blocks.11.mlp.c_proj.bias, Parameters: 768\n",
            "Layer: ln_f.weight, Parameters: 768\n",
            "Layer: ln_f.bias, Parameters: 768\n",
            "Total Trainable Parameters: 124439808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warmup_steps = 10      # learning rate increase upto this step\n",
        "max_learning_rate = 6e-4\n",
        "min_learning_rate = 0.1 * max_learning_rate\n",
        "max_steps = 50\n",
        "\n",
        "def get_learning_rate(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_learning_rate * (it + 1) / warmup_steps\n",
        "\n",
        "    if it > max_steps:\n",
        "        return min_learning_rate\n",
        "\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_learning_rate + coeff * (max_learning_rate - min_learning_rate)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4, betas = (0.9, 0.95), eps = 1e-8)\n",
        "optimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device = device)\n",
        "\n",
        "\n",
        "# In order to simulate GPT-3 0.5M batch size\n",
        "total_batch_size = 524288\n",
        "assert total_batch_size % (B * T) == 0, \"check total_batch_size portion again\"\n",
        "grad_accumulation_steps = total_batch_size // (B * T)\n",
        "print(f\"Gradient accumulation steps: {grad_accumulation_steps}\")\n",
        "print(f\"=> calculated gradient accumulation steps = {grad_accumulation_steps}\")\n",
        "\n",
        "\n",
        "\n",
        "# try to overfit model over single batch\n",
        "train_loader = DataLoader(B = batch_size, T = block_size)\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "\n",
        "x, y = train_loader.next_batch()\n",
        "x, y = x.to(device), y.to(device)\n",
        "\n",
        "\n",
        "for step in range(50):\n",
        "#     if (step % 25 == 0):\n",
        "#         losses = estimate_loss()\n",
        "#         print(f\"step: {step}, train loss: {losses['train']}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accumulation_steps):\n",
        "        x_batch, y_batch = x, y\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "\n",
        "        logits, loss = model(x_batch, y_batch)\n",
        "        loss = loss / grad_accumulation_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    learning_rate = get_learning_rate(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = learning_rate\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    T = (time.time() - t0) * 1000\n",
        "    tokens_per_second = (train_loader.B * train_loader.T) / T * 1000\n",
        "    print(f\"Step {step}, Loss: {loss.item()} | Learning Rate = {learning_rate:.4f} | Norm = {norm:.4f} | Time: {T:.2f}ms | tok/sec = {tokens_per_second}\")\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "Xa3YSdBt-qao",
        "outputId": "1380f613-3037-4866-a564-ae22d3d9a151"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1395481 tokens\n",
            "1 epoch = 340 batches\n",
            "Step 0, Loss: 7.190181255340576, Time: 1172.14ms, tok/sec = 3494.4513840410973\n",
            "Step 1, Loss: 8.258306503295898, Time: 1000.24ms, tok/sec = 4095.021718082066\n",
            "Step 2, Loss: 7.2768354415893555, Time: 992.76ms, tok/sec = 4125.855691454731\n",
            "Step 3, Loss: 6.7461113929748535, Time: 993.98ms, tok/sec = 4120.821384299503\n",
            "Step 4, Loss: 10.635149002075195, Time: 1000.34ms, tok/sec = 4094.602039785153\n",
            "Step 5, Loss: 6.351304054260254, Time: 1003.89ms, tok/sec = 4080.1252413241464\n",
            "Step 6, Loss: 6.2443413734436035, Time: 1007.30ms, tok/sec = 4066.3249595908837\n",
            "Step 7, Loss: 6.077468395233154, Time: 1003.48ms, tok/sec = 4081.8061925605543\n",
            "Step 8, Loss: 5.961909294128418, Time: 1013.84ms, tok/sec = 4040.100580927617\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f3502d8d46f0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# ______________________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m     \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtokens_per_second\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    983\u001b[0m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_synchronize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(B = batch_size, T = block_size, split = 'train')\n",
        "\n",
        "\n",
        "for iter in range(100):\n",
        "    # if (iter % eval_interval == 0):\n",
        "    #     losses = estimate_loss()\n",
        "    #     print(f\"step: {iter}, train loss: {losses['train']}, val loss: {losses['val']:.4f}\")\n",
        "\n",
        "    t0 = time.time()\n",
        "    x_batch, y_batch = train_loader.next_batch()\n",
        "    x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x_batch, y_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    T = (time.time() - t0) * 1000\n",
        "    print(f\"Step {iter}, Loss: {loss.item()}, Time: {T:.2f}ms\")\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "jCv_k4mqoWfT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ac9042f-2849-4000-cf67-0b17a1068d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.053647518157959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'gpt2_model.pth')\n"
      ],
      "metadata": {
        "id": "vXRaCmmRS4RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(vocab_size)  # Initialize model architecture\n",
        "\n",
        "# If using CPU\n",
        "# model.load_state_dict(torch.load('ngram_language_model.pth', map_location=torch.device('cpu')))\n",
        "\n",
        "# If using GPU\n",
        "model.load_state_dict(torch.load('gpt2_model.pth'))  # Load saved parameters\n",
        "model = model.to(device)  # Move to GPU if available"
      ],
      "metadata": {
        "id": "S_XJtJhCVt7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FINAL CODE**"
      ],
      "metadata": {
        "id": "G9-fDUINMztP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and load dataset\n",
        "!wget -O input.txt https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
        "\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "\n",
        "# Imports\n",
        "!pip install tiktoken\n",
        "\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "# Custom Dataloader\n",
        "class DataLoader:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f\"Loaded {len(tokens)} tokens\")\n",
        "        print(f\"1 epoch = {len(tokens) // (B * T)} batches\")\n",
        "\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buffer = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
        "        x = (buffer[:-1]).view(B, T) # inputs\n",
        "        y = (buffer[1:]).view(B, T) # targets\n",
        "\n",
        "        self.current_position += B * T\n",
        "\n",
        "        if self.current_position + B * T > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n",
        "\n",
        "# Hyperparameters and device selection\n",
        "n_embed = 768          # Size of embeddings\n",
        "n_head = 12            # Number of attention heads\n",
        "n_layer = 12           # Number of transformer layers\n",
        "batch_size = 4         # Batch size\n",
        "block_size = 1024      # Context length\n",
        "max_iters = 1005       # Training iterations\n",
        "eval_interval = 500    # Evaluation interval\n",
        "learning_rate = 5e-4   # Learning rate\n",
        "eval_iters = 200       # Evaluation steps for validation\n",
        "dropout = 0.1          # Dropout probability\n",
        "\n",
        "vocab_size = 50304     # Vocabulary size :: Originally = 50257, update to nice number 50304 -> divisible by 128\n",
        "\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "\n",
        "\n",
        "# GPT Model\n",
        "class CasualSelfAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        assert n_embed % n_head == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.n_embed = n_embed\n",
        "\n",
        "        self.c_attn = nn.Linear(n_embed, 3 * n_embed) # for all 3 (key, query, value)\n",
        "        self.c_proj = nn.Linear(n_embed, n_embed)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embed, dim = 2) # 3 -> (B, T, n_embed)\n",
        "\n",
        "        head_size = C // self.n_head\n",
        "        q = q.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        k = k.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, head_size).transpose(1, 2)\n",
        "\n",
        "        # Flash Attention\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal = True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # Reshape back to (B, T, n_embed)\n",
        "        y = self.c_proj(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(n_embed, 4 * n_embed)\n",
        "        self.gelu = nn.GELU()\n",
        "        self.c_proj = nn.Linear(4 * n_embed, n_embed)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        head_size = n_embed // n_head\n",
        "        # self.attention = MultiHeadAttention(n_head, head_size) # 4 heads of 8-dimensional self-attention\n",
        "        # self.fforward = FeedForward(n_embed)\n",
        "        self.ln1 = nn.LayerNorm(n_embed)\n",
        "        self.attention = CasualSelfAttention()\n",
        "        self.ln2 = nn.LayerNorm(n_embed)\n",
        "        self.mlp = MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attention(self.ln1(x))\n",
        "        # x = x + self.fforward(self.ln2(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
        "        # self.blocks = nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embed)\n",
        "        self.language_model_head = nn.Linear(n_embed, vocab_size, bias = False)\n",
        "\n",
        "        self.language_model_head.weight = self.token_embedding_table.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.size()\n",
        "        assert T <= block_size, f\"Sequence length {T} exceeds block size {block_size}\"\n",
        "\n",
        "        token_embeddings = self.token_embedding_table(idx) #(Batch, Time, Channel)\n",
        "        position_embeddings = self.position_embedding_table(torch.arange(0, T, dtype = torch.long, device = idx.device)) #(Time, Channel)\n",
        "        x = token_embeddings + position_embeddings\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "\n",
        "        logits = self.language_model_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx -> (B, T)\n",
        "        with torch.no_grad():\n",
        "            for i in range(max_new_tokens):\n",
        "                idx_cond = idx[:, -block_size:] # (B, T)\n",
        "                logits, loss = self(idx_cond) # (B, T, C)\n",
        "                logits = logits[:, -1, :] # last time step only | becomes (B, C)\n",
        "                prob = F.softmax(logits, dim = -1)\n",
        "                idx_next = torch.multinomial(prob, num_samples = 1) # predicted | (B, 1)\n",
        "                idx = torch.cat((idx, idx_next), dim = 1) # (B, T + 1)\n",
        "        return idx\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        if master_process:\n",
        "            print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "            print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "\n",
        "        if master_process:\n",
        "            print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n",
        "\n",
        "\n",
        "model = GPT(vocab_size)\n",
        "model = model.to(device = device)\n",
        "model = torch.compile(model) # Works with A100, not much help for T4\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")\n",
        "\n",
        "\n",
        "\n",
        "# Training loop\n",
        "warmup_steps = 10      # learning rate increase upto this step\n",
        "max_learning_rate = 6e-4\n",
        "min_learning_rate = 0.1 * max_learning_rate\n",
        "max_steps = 50\n",
        "\n",
        "def get_learning_rate(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_learning_rate * (it + 1) / warmup_steps\n",
        "\n",
        "    if it > max_steps:\n",
        "        return min_learning_rate\n",
        "\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_learning_rate + coeff * (max_learning_rate - min_learning_rate)\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4, betas = (0.9, 0.95), eps = 1e-8)\n",
        "optimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate = 6e-4, device = device)\n",
        "\n",
        "\n",
        "# In order to simulate GPT-3 0.5M batch size\n",
        "total_batch_size = 524288\n",
        "assert total_batch_size % (B * T) == 0, \"check total_batch_size portion again\"\n",
        "grad_accumulation_steps = total_batch_size // (B * T)\n",
        "print(f\"Gradient accumulation steps: {grad_accumulation_steps}\")\n",
        "print(f\"=> calculated gradient accumulation steps = {grad_accumulation_steps}\")\n",
        "\n",
        "\n",
        "\n",
        "# try to overfit model over single batch\n",
        "train_loader = DataLoader(B = batch_size, T = block_size)\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "\n",
        "for step in range(50):\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accumulation_steps):\n",
        "        x_batch, y_batch = train_loader.next_batch()\n",
        "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "\n",
        "        logits, loss = model(x_batch, y_batch)\n",
        "        loss = loss / grad_accumulation_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    learning_rate = get_learning_rate(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = learning_rate\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "    T = (time.time() - t0) * 1000\n",
        "    tokens_per_second = (train_loader.B * train_loader.T) / T * 1000\n",
        "    print(f\"Step {step}, Loss: {loss.item()} | Learning Rate = {learning_rate:.4f} | Norm = {norm:.4f} | Time: {T:.2f}ms | tok/sec = {tokens_per_second}\")\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "id": "STdbFUElPHVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2n5b4V5S48q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}